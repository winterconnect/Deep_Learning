{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "037_NLP_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDnfoEeY2qdp"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zt5yQd120Yf"
      },
      "source": [
        "import nltk\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vq3he9X6Vde"
      },
      "source": [
        "!pip install konlpy\n",
        "# konlpy 안에 형태소 분석기 여러개가 들어있다\n",
        "# 형태소 분석기에 따라 성능 차이가 있으므로, 비교해보며 사용해보아야 한다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rXyiEQ16bGK"
      },
      "source": [
        "from konlpy.tag import Okt, Kkma"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ollM9IWW7h9p"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Gk8iok2t-L"
      },
      "source": [
        "### 1. Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN0C1jyV2wmh"
      },
      "source": [
        "### 1) 영어: NLTK(Natural Language Toolkit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axI0sHxb23Vu",
        "outputId": "c7551a79-3f2c-4c5c-cb7f-31ea34906b6b"
      },
      "source": [
        "nltk.download('punkt')\n",
        "# nltk는 한국어 정보는 없다"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx4UB1qr2_jK"
      },
      "source": [
        "#### (1) 문장 토큰화: sent_tokenize()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y8WVjdf3Gxz"
      },
      "source": [
        "sentences = 'The X-Files is an American science fiction \\\n",
        " drama television series created by Chris Carter. \\\n",
        " The original television series aired from September 10, 1993 \\\n",
        " to May 19, 2002 on Fox. The program spanned nine seasons, with 202 episodes.'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr8PQP-g3asP",
        "outputId": "1275b1eb-84b0-4485-9455-021c8edeeb37"
      },
      "source": [
        "sent_tokenize(sentences)\n",
        "# 문장단위 리스트로 리턴"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The X-Files is an American science fiction  drama television series created by Chris Carter.',\n",
              " 'The original television series aired from September 10, 1993  to May 19, 2002 on Fox.',\n",
              " 'The program spanned nine seasons, with 202 episodes.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXjXcho33mAY"
      },
      "source": [
        "#### (2) 단어 토큰화: word_tokenize()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeU7GmVIvP3z"
      },
      "source": [
        "text = 'The truth is out there.'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ-MTvNE3ssU",
        "outputId": "62d5b87b-0318-48b3-90bc-42fbee51b580"
      },
      "source": [
        "word_tokenize(text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'truth', 'is', 'out', 'there', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXJJW4UF3v3u"
      },
      "source": [
        "#### (3) 단어품사(Part of Speech) 태깅: pos_tag()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8SiOG2D34Ih",
        "outputId": "8e9374ba-3fa4-4abe-9105-8a1f0cdf381a"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CjsDOBF38EM",
        "outputId": "0996afd0-57bd-49de-d601-c016b92689af"
      },
      "source": [
        "x = word_tokenize(text)\n",
        "\n",
        "pos_tag(x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('truth', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('out', 'RP'),\n",
              " ('there', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E39MYk814Deb"
      },
      "source": [
        "#### (4) Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDeR5zzM4GuU",
        "outputId": "601c5127-b6e5-403f-d822-68af27fb6953"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsSD5i354KfN"
      },
      "source": [
        "- 'English' Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNJ_qTk34NwQ",
        "outputId": "aa398e73-04da-49c8-ee6a-8299846de7b7"
      },
      "source": [
        "print('English Stop Words: ' , len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Stop Words:  179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ln4w0Im4W8k"
      },
      "source": [
        "- tokenize_text() 정의\n",
        "  - 여러개의 문장별로 단어 토큰 생성 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgvWoSRr4cxB"
      },
      "source": [
        "def tokenize_text(doc) :\n",
        "  sentences = sent_tokenize(doc)\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "  return word_tokens\n",
        "  # 문서에서 문장을 뽑아내서, 문장에서 워드를 뽑아냄"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkvZ-CL04nt5"
      },
      "source": [
        "- 문장별 단어 토큰화 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxqhw_7DwAeu",
        "outputId": "2ea8dd8d-d113-4b11-be86-3da18a029179"
      },
      "source": [
        "word_tokens = tokenize_text(sentences)\n",
        "\n",
        "print(type(word_tokens), len(word_tokens))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knTTdVf64wjM"
      },
      "source": [
        "- 문장별 단어 토큰화 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3wER6I14y_u",
        "outputId": "26020899-6dce-496c-9e85-17130fed18e3"
      },
      "source": [
        "print(word_tokens)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', 'X-Files', 'is', 'an', 'American', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'by', 'Chris', 'Carter', '.'], ['The', 'original', 'television', 'series', 'aired', 'from', 'September', '10', ',', '1993', 'to', 'May', '19', ',', '2002', 'on', 'Fox', '.'], ['The', 'program', 'spanned', 'nine', 'seasons', ',', 'with', '202', 'episodes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAzStV1b41gn"
      },
      "source": [
        "- Stop Words 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSTT3ud44kk"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens :\n",
        "  filtered_words = []\n",
        "\n",
        "  for word in sentence :\n",
        "    word = word.lower()\n",
        "    if word not in stopwords :\n",
        "      filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6XlW9Ql5I-O"
      },
      "source": [
        "- Stop Words 처리 결과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBqSPA_K5LLW",
        "outputId": "13609327-b551-4ca8-ea48-e016cc698763"
      },
      "source": [
        "print(all_tokens)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KnmZ1F35NTM"
      },
      "source": [
        "#### (5) Stemming(어간 추출)\n",
        "- 변화된 단어의 원형으로 처리\n",
        "  - work\n",
        "  - amuse\n",
        "  - happy\n",
        "  - fancy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0a3BGYk5W4X",
        "outputId": "7cd34ef9-d63f-4be1-e610-bc032a0ef617"
      },
      "source": [
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working') , stemmer.stem('works') , stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing') , stemmer.stem('amuses') , stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier') , stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier') , stemmer.stem('fanciest'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meV7mCyY5oVS"
      },
      "source": [
        "#### (6) Lemmatization(표제어 추출)\n",
        "- 변화된 단어의 원형을 처리\n",
        "  - Stemming 보다 정확한 처리 가능\n",
        "  - '품사'를 지정하여 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLvGKb9F5vq3",
        "outputId": "fef9632b-cf04-401b-da7f-bc8febaa0188"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBiSjaY055eT",
        "outputId": "7f261cac-6ac9-4b21-ddaf-3661ef879d4b"
      },
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('amusing' , 'v') , lemma.lemmatize('amuses' , 'v') ,\n",
        "      lemma.lemmatize('amused' , 'v'))\n",
        "print(lemma.lemmatize('happier' , 'a') , lemma.lemmatize('happiest' , 'a'))\n",
        "print(lemma.lemmatize('fancier' , 'a') , lemma.lemmatize('fanciest' , 'a')) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp-0PLn6PdP"
      },
      "source": [
        "### 2) 한국어: KoNLPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_BvWD5a6R1Q"
      },
      "source": [
        "#### (1) Okt 형태소 분석기(Open Korea Text, Twitter)\n",
        "- 형태소(Morpheme)\n",
        "- Okt: 트위터 데이터를 수집해서 학습시킨 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbzASGRr62L_"
      },
      "source": [
        "- 토큰화: okt.morphs()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84LEq5SvwIjg",
        "outputId": "a9f283ca-e4aa-449b-ca60-1104399a7615"
      },
      "source": [
        "okt = Okt()\n",
        "\n",
        "print(okt.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례없는 고통을 겪으며 다양한 방식으로 심각하게 피해르 겪었습니다.'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['지난', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인해', '전례', '없는', '고통', '을', '겪으며', '다양한', '방식', '으로', '심각하게', '피해', '르', '겪었습니다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "who4Up0763FR"
      },
      "source": [
        "- 품사 태깅: okt.pos()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlQgBoir66u0",
        "outputId": "e7bb36c3-06e6-45ba-edd9-ab9b14bdff48"
      },
      "source": [
        "print(okt.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례없는 고통을 겪으며 다양한 방식으로 심각하게 피해르 겪었습니다.'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('지난', 'Noun'), ('몇', 'Noun'), ('달', 'Noun'), ('간', 'Suffix'), ('전', 'Noun'), ('세계', 'Noun'), ('모든', 'Noun'), ('사람', 'Noun'), ('은', 'Josa'), ('코로나', 'Noun'), ('19', 'Number'), ('로', 'Noun'), ('인해', 'Adjective'), ('전례', 'Noun'), ('없는', 'Adjective'), ('고통', 'Noun'), ('을', 'Josa'), ('겪으며', 'Verb'), ('다양한', 'Adjective'), ('방식', 'Noun'), ('으로', 'Josa'), ('심각하게', 'Adjective'), ('피해', 'Noun'), ('르', 'Noun'), ('겪었습니다', 'Verb'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9DzedKD6_K-"
      },
      "source": [
        "- 명사 추출: okt.nouns()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQh7PrvG7CWl",
        "outputId": "34cb9751-f70d-47c4-bcea-a1c9288e104b"
      },
      "source": [
        "print(okt.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례없는 고통을 겪으며 다양한 방식으로 심각하게 피해르 겪었습니다.'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['지난', '몇', '달', '전', '세계', '모든', '사람', '코로나', '로', '전례', '고통', '방식', '피해', '르']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfi9DdRG7FRV"
      },
      "source": [
        "#### (2) Kkma 형태소 분석기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-tHXEDe7MDo"
      },
      "source": [
        "- 토큰화: kkma.morphs()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu7WjD5z7O-2"
      },
      "source": [
        "# Kkma(꼬꼬마) 형태소 분석기\n",
        "kkma = Kkma()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSdCdwMk7Qib",
        "outputId": "2b2fa3c8-7d69-41b0-8b29-716ec815f93b"
      },
      "source": [
        "print(kkma.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례없는 고통을 겪으며 다양한 방식으로 심각하게 피해르 겪었습니다.'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['지나', 'ㄴ', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인하', '어', '전례', '없', '는', '고통', '을', '겪', '으며', '다양', '하', 'ㄴ', '방식', '으로', '심각', '하', '게', '피해', '르', '겪', '었', '습니다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKkeMhzL7T-l"
      },
      "source": [
        "- 품사태깅: kkma.pos()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh72c6Ry7V5o",
        "outputId": "37523251-2793-48c1-b426-8ad03c9a591d"
      },
      "source": [
        "print(kkma.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례없는 고통을 겪으며 다양한 방식으로 심각하게 피해르 겪었습니다.'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('지나', 'VV'), ('ㄴ', 'ETD'), ('몇', 'MDT'), ('달', 'NNG'), ('간', 'NNB'), ('전', 'NNG'), ('세계', 'NNG'), ('모든', 'MDT'), ('사람', 'NNG'), ('은', 'JX'), ('코로나', 'NNG'), ('19', 'NR'), ('로', 'JKM'), ('인하', 'VV'), ('어', 'ECS'), ('전례', 'NNG'), ('없', 'VA'), ('는', 'ETD'), ('고통', 'NNG'), ('을', 'JKO'), ('겪', 'VV'), ('으며', 'ECE'), ('다양', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('방식', 'NNG'), ('으로', 'JKM'), ('심각', 'XR'), ('하', 'XSA'), ('게', 'ECD'), ('피해', 'NNG'), ('르', 'NNG'), ('겪', 'VV'), ('었', 'EPT'), ('습니다', 'EFN'), ('.', 'SF')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa7gA9QN7ZVC"
      },
      "source": [
        "- 명사추출: kkma.nouns()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ2Q08nI7cgb",
        "outputId": "cc774d8e-0a54-4f47-b52e-5816d1d155b1"
      },
      "source": [
        "print(kkma.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례없는 고통을 겪으며 다양한 방식으로 심각하게 피해르 겪었습니다.'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['달', '달간', '간', '전', '세계', '사람', '코로나', '코로나19', '19', '전례', '고통', '다양', '방식', '피해', '피해르', '르']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZTlx_0u7fTT"
      },
      "source": [
        "## 2. Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4sXMjjN7oN5"
      },
      "source": [
        "sentence = '가지마라 가지마라 그녀는 위험해 매력이 너무 넘치는 Girl \\\n",
        "하지마라 하지마라 사랑은 위험해 \\\n",
        "내가 내가 내가 먼저 네게 네게 네게 빠져 빠져 빠져 버려 baby'"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcF__3uw7xVP"
      },
      "source": [
        "### 1) 정수인코딩\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4DT8eOw7zak"
      },
      "source": [
        "#### (1) Tokenizer.fit_on_texts()\n",
        "- tokenization & Integer Indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDmpusKr76FI"
      },
      "source": [
        "# keras tokenizer 사용\n",
        "tknz = Tokenizer()\n",
        "tknz.fit_on_texts([sentence])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd3gyIVB79nC",
        "outputId": "a862ed90-a955-4d89-b30d-0d8c5c43bbfd"
      },
      "source": [
        "print(tknz.word_index)\n",
        "# 정수로 tokenizing 함"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'내가': 1, '네게': 2, '빠져': 3, '가지마라': 4, '위험해': 5, '하지마라': 6, '그녀는': 7, '매력이': 8, '너무': 9, '넘치는': 10, 'girl': 11, '사랑은': 12, '먼저': 13, '버려': 14, 'baby': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeiF5XDm8AOm"
      },
      "source": [
        "#### (2) Tokenizer.texts_to_sequences()\n",
        "- Integer Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6E94BG8HX7v"
      },
      "source": [
        "LBE = tknz.texts_to_sequences([sentence])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up5CPC5_HbXW",
        "outputId": "eb79ab57-ff1a-40df-d802-923eb2ec29ae"
      },
      "source": [
        "print(LBE)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 4, 7, 5, 8, 9, 10, 11, 6, 6, 12, 5, 1, 1, 1, 13, 2, 2, 2, 3, 3, 3, 14, 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnq_3rUoHfzz"
      },
      "source": [
        "### 3) 원-핫 인코딩(One-Hot Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82achneHilc"
      },
      "source": [
        "#### (1) to_categorical()\n",
        "- One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkjWkBevHnMC"
      },
      "source": [
        "OHE = to_categorical(LBE)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HFXlIO9HpKi",
        "outputId": "5a75b67e-558d-4a98-ee85-793c7e34eb19"
      },
      "source": [
        "print(OHE)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}